---
bibliography: references.bib
---

# Importance et notions de base

## Rappels sur les notions de base

### Types de statistiques

-   **Descriptives**: le but est d'illustrer les données;\
-   **Paramétriques**: on suppose que la/les variables(s) suivent une distribution particulière;\
-   **Non paramétriques**: on ne fait aucune supposition concernant la distribution de la/les variables(s).

### Variables

-   Une variable est un attribut mesuré pour chaque individu/observation.\
-   Types d'échelle de mesure:
    -   Ratio: le 0 est clairement défini; ne peut pas être \< 0.\
    -   Intervalle: le 0 est arbitraire; peut être \< 0.\
    -   Ordinale: l'ordre est défini; les rangs n'indiquent pas nécessairement des différentes constantes.\
    -   Nominale/Catégorique: l'ordre est non défini.\
    -   Continue: présente une infinité de valeurs possibles.\
    -   Discrète: nombre limité de valeurs possibles.

### Qu'est-ce qu'une distribution?

Une fonction qui exprime la fréquence (densité de probabilité) des différentes valeurs pouvant être prise par une variable.

Rappelons ici quelques distributions usuelles.

#### Distribution uniforme (continue)

Bien que les origines historiques de la conception de la distribution uniforme ne soient pas concluantes, on suppose que le terme "uniforme" est né du concept d'équiprobabilité dans les jeux de dés (notez que les jeux de dés auraient un espace d'échantillonnage uniforme discret et non continu).

Les paramètres de la distribution uniforme sont:

-   Moyenne: $\frac{1}{2}(a + b)$.\
-   Variance: $\frac{1}{12}(b - a)^2$.

La fonction de densité de la loi uniforme est:

$$
f(x) = \left\{\begin{array}{ll}\frac{1}{b - a} & \mbox{for } a \leq x \leq b, \\ 0 & \mbox{otherwise}\end{array}\right.
$$

```{r, echo=FALSE}
# Load necessary library
library(ggplot2)

# Define a function to plot the uniform distribution
plot_uniform_distribution <- function(mins, maxs, xlim = c(-1, 1), points = 1000) {
  # Create a data frame to hold the x values and densities for each min and max
  data <- data.frame()
  
  # Loop through each combination of min and max
  for (min_val in mins) {
    for (max_val in maxs) {
      x <- seq(xlim[1], xlim[2], length.out = points)
      y <- ifelse(x >= min_val & x <= max_val, 1 / (max_val - min_val), 0)
      temp <- data.frame(x = x, y = y, min = factor(min_val), max = factor(max_val))
      data <- rbind(data, temp)
    }
  }
  
  # Plot using ggplot2
  p <- ggplot(data, aes(x = x, y = y, color = min, linetype = max)) +
    geom_line() +
    labs(title = "Uniform Distribution",
         x = "x",
         y = "Density",
         color = "Min",
         linetype = "Max") +
    theme_minimal()
  
  print(p)
}

# Define min and max values
mins <- c(-2)
maxs <- c(2)

# Plot the uniform distributions
plot_uniform_distribution(mins, maxs, xlim = c(-4, 4))
```

#### Distribution binomiale

La distribution binomiale est fréquemment utilisée pour modéliser le nombre de succès dans un échantillon de taille $n$ tiré avec remplacement d'une population de taille $N$.

Les paramètres de la distribution binomiale sont:

-   Moyenne: $\mu = np$.

-   Variance: $\sigma^2 = npq$.

La fonction de densité de la loi binomiale est:

$$
f(k, n, p) = \binom{n}{k}p^k (1-p)^{n-k}
$$

```{r, echo=FALSE}
# Load necessary libraries
library(ggplot2)

# Define a function to plot the binomial PMF
plot_binomial_pmf <- function(n, p) {
  # Create a data frame to hold the values and probabilities
  x <- 0:n
  y <- dbinom(x, size = n, prob = p)
  data <- data.frame(x = x, y = y)
  
  # Plot using ggplot2
  p <- ggplot(data, aes(x = factor(x), y = y)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "blue") +
    labs(title = paste("Binomial Distribution PMF (n =", n, ", p =", p, ")"),
         x = "Number of Successes",
         y = "Probability") +
    theme_minimal()
  
  print(p)
}

# Define parameters for the binomial distribution
n <- 10  # number of trials
p <- 0.5 # probability of success in each trial

# Plot the binomial PMF
plot_binomial_pmf(n, p)
```

#### Distribution de Poisson

La distribution de Poisson est une distribution de probabilité discrète qui exprime la probabilité qu'un nombre donné d'événements se produisent dans un intervalle de temps fixe si ces événements se produisent avec un taux moyen constant connu et indépendamment du temps écoulé depuis le dernier événement.

Les paramètres de la distribution de Poisson sont:

-   Moyenne: $\mu = \lambda$

-   Variance: $\sigma^2 = \lambda$

La fonction de densité de probabilité de la distribution de Poisson est:

$$
f(k; \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}
$$

```{r, echo=FALSE}
# Load necessary libraries
library(ggplot2)

# Define a function to plot the Poisson PMF
plot_poisson_pmf <- function(lambda, max_x = 15) {
  # Create a data frame to hold the values and probabilities
  x <- 0:max_x
  y <- dpois(x, lambda = lambda)
  data <- data.frame(x = x, y = y)
  
  # Plot using ggplot2
  p <- ggplot(data, aes(x = factor(x), y = y)) +
    geom_bar(stat = "identity", fill = "skyblue", color = "blue") +
    labs(title = paste("Poisson Distribution PMF (lambda =", lambda, ")"),
         x = "Number of Events",
         y = "Probability") +
    theme_minimal()
  
  print(p)
}

# Define parameter for the Poisson distribution
lambda <- 7  # average rate (mean)

# Plot the Poisson PMF
plot_poisson_pmf(lambda)
```

#### Distribution normale

Les distributions normales sont importantes en statistique et sont souvent utilisées dans les sciences naturelles et sociales pour représenter des variables aléatoires à valeur réelle dont la distribution n'est pas connue. Leur importance est en partie due au théorème de la limite centrale. Ce théorème stipule que, sous certaines conditions, la moyenne de nombreux échantillons (observations) d'une variable aléatoire de moyenne et de variance finies est elle-même une variable aléatoire, dont la distribution converge vers une distribution normale à mesure que le nombre d'échantillons augmente. Par conséquent, les quantités physiques qui sont censées être la somme de nombreux processus indépendants, tels que les erreurs de mesure, ont souvent des distributions qui sont presque normales.

Les paramètres de la distribution normale sont:

-   Moyenne: $\mu = \frac{\sum_{i=1}^{N} X_i}{N}$.
-   Variance: $\sigma^2 = \frac{\sum (X_i - \mu)^2}{N}$.

La fonction de densité de la loi normale est:

$$ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

```{r, echo=FALSE}
# Load necessary libraries
library(ggplot2)

# Define a function to plot the normal distribution
plot_normal_distribution <- function(means, variances, xlim = c(-10, 10), points = 1000) {
  # Create a data frame to hold the x values and densities for each mean and variance
  data <- data.frame()
  
  # Loop through each combination of mean and variance
  for (mean in means) {
    for (variance in variances) {
      sd <- sqrt(variance)
      x <- seq(xlim[1], xlim[2], length.out = points)
      y <- dnorm(x, mean = mean, sd = sd)
      temp <- data.frame(x = x, y = y, mean = factor(mean), variance = factor(variance))
      data <- rbind(data, temp)
    }
  }
  
  # Plot using ggplot2
  p <- ggplot(data, aes(x = x, y = y, color = mean, linetype = variance)) +
    geom_line() +
    labs(title = "Normal Distribution",
         x = "x",
         y = "Density",
         color = "Mean",
         linetype = "Variance") +
    theme_minimal()
  
  print(p)
}

# Define means and variances
means <- c(0, 2, -2)
variances <- c(1, 2, 0.5)

# Plot the normal distributions
plot_normal_distribution(means, variances)
```

#### Distribution $t$ de Student

En probabilité et en statistique, la distribution $t$ de Student (ou simplement la distribution $t$) $t_\nu$ est une distribution de probabilité continue qui généralise la distribution normale standard. Comme cette dernière, elle est symétrique autour de zéro et en forme de cloche.

Les paramètres de la distribution $t$ de Student sont:

-   Moyenne: 0 pour $\nu > 1$ et indéfini dans le cas contraire.

-   Variance: $\frac{\nu}{\nu - 2}$ pour $\nu > 2$, $\infty$ pour $1 < \nu \leq 2$, indéfini dans les cas contraires.

La fonction de densité de probabilité de la distribution de $t$ est:

$$
f(t) = \frac{\Gamma(\frac{\nu + 1}{2})}{\sqrt{\pi \nu} \Gamma(\frac{\nu}{2})} (1 + \frac{t^2}{\nu}) ^ {-(\nu + 1)/2}
$$

ou $\nu$ est le nombre de dégrés de libertés et $\Gamma$ est la fonction Gamma.

```{r, echo=FALSE}
# Load necessary libraries
library(ggplot2)

# Define a function to plot the PDF of the Student's t-distribution
plot_t_pdf <- function(df, x_range = c(-5, 5)) {
  # Create a sequence of x values
  x <- seq(x_range[1], x_range[2], length.out = 1000)
  y <- dt(x, df = df)
  
  # Create a data frame
  data <- data.frame(x = x, y = y)
  
  # Plot using ggplot2
  p <- ggplot(data, aes(x = x, y = y)) +
    geom_line(color = "blue") +
    labs(title = paste("Student's t-Distribution PDF (df =", df, ")"),
         x = "x",
         y = "Density") +
    theme_minimal()
  
  print(p)
}

# Define degrees of freedom for the t-distribution
df <- 5  # degrees of freedom

# Plot the t-distribution PDF
plot_t_pdf(df)
```

La distribution t de Student avec $\nu$ degrés de liberté peut être définie comme la distribution de la variable aléatoire $T$ avec $$T = \frac{Z}{\sqrt{V/\nu}} = Z\sqrt{\frac{\nu}{V}}$$

avec

-   $Z$ est une normale standard avec une valeur attendue de 0 et une variance de 1 ;

-   $V$ suit une distribution de khi-deux ($\chi^2$) avec $\nu$ dégrés de liberté;

-   $Z$ et $V$ sont indépendants.

## Pourquoi a-t-on besoin de méthodes statistiques ?

La plupart des sciences sont comparatives. Les chercheurs ont souvent besoin de savoir si un traitement expérimental particulier a eu un effet, ou s'il existe des différences entre une variable particulière mesurée à plusieurs endroits différents. Par exemple, un nouveau médicament a-t-il un effet sur la tension artérielle, un régime riche en vitamine C réduit-il le risque de cancer du foie chez l'homme, ou existe-t-il une relation entre la couverture végétale et la densité de la population de lapins ? Mais lorsque vous faites ce genre de comparaisons, les différences entre les traitements ou entre les zones échantillonnées peuvent être réelles ou peuvent simplement être le type de variation qui se produit par hasard entre les échantillons d'une même population.

Prenons 2 situations concrètes:

-   Dans la première, nous jouons à pile ou face avec 10 pièces. On obtient après un lancer unique de chacune des pièces 3 faces et 7 piles. Pouvons-nous dire si ce lot de pièces est truqué?\
-   Dans la seconde, on échantillone une population de singes, et on observe 32 femelles et 54 mâles. Y a-t-il réellement plus de mâles dans la population?

**Les probabilités vous aident à prendre une décision concernant vos résultats.**

### Tests statistiques et niveaux de signification

Un test statistique est une aide à la décision concernant une hypothèse (alternative *vs* nulle).

L'idée maîtresse à la base de tout test d'hypothèse statistique:

-   Je suppose que $H_0$ (hypothèse nulle) est vraie (pas de différence/relation);\
-   Je calcule la p-valeur, c'est à dire la probabilité d'obtenir une différence/relation au moins aussi forte que celle que j'ai observée dans mon échantillon (qui est un fait établi);\
-   Si cette p-valeur est faible, alors il est probable que $H_0$ soit fausse: la différence mesurée est simplement une mauvaise estimation d'une différence nulle.

Le statisticien Sir Ronald Fisher a proposé que, si la probabilité d'obtenir cette différence ou une différence plus extrême entre le résultat attendu et le résultat réel est inférieure à 5 %, il convient de conclure que la différence est statistiquement significative (Fisher, 1954). Le choix de 5% (qui correspond à 1/20 ou 0,05) n'a pas de raison biologique. C'est la probabilité que de nombreux chercheurs utilisent comme « niveau de signification statistique » standard.

::: callout-warning
Attention: ce seuil n'est pas une valeur magique, les statistiques ce n'est pas blanc *vs* noir, vrai *vs* faux, significatif *vs* non significatif...
:::

### Exemple de calcul direct de la p-valeur

Prenons un exemple de sac contenant 1000 boules blanches et noires. On en tire 6 au hasard [@mckillup2011]. On veut tester l'hypothèse qu'il y a 500 blanches et 500 noires: $H_0$: $\mathbb{P}(blanche) = 0,5$ *vs* $H_A$: $\mathbb{P}(blanche) ≠ 0,5$.

::: callout-note
## Concepts basiques de probabilité

La probabilité d'un événement ne peut varier qu'entre 0 et 1 (qui correspondent à 0 et 100%). Si un événement est certain de se produire, il a une probabilité de 1 ; alors que s'il est certain que l'événement ne se produira pas, il a une probabilité de 0.

La probabilité d'un événement particulier est le nombre de résultats donnant cet événement, divisé par le nombre total de résultats possibles. Par exemple, lorsque vous jouez à pile ou face, il n'y a que deux résultats possibles : pile ou face. Ces deux événements s'excluent mutuellement : il est impossible d'obtenir les deux.

Par conséquent, la probabilité d'obtenir un résultat positif est de 1 divisé par 2 = ½ (et donc la probabilité d'obtenir un résultat négatif est également de ½).

**La règle de l'addition**

La probabilité d'obtenir une tête ou une queue est de ½ + ½ = 1. Il s'agit d'un exemple de la règle de l'addition : lorsque plusieurs résultats s'excluent mutuellement, la probabilité d'obtenir l'un d'entre eux est la somme de leurs probabilités respectives. (Par conséquent, la probabilité d'obtenir un 1, un 2, un 3 ou un 4 en lançant un dé à six faces est de 4/6).

**La règle de multiplication**

**Événements indépendants.** Pour calculer la probabilité conjointe de deux ou plusieurs événements indépendants (par exemple, un face suivi d'un autre face lors de deux lancers indépendants d'une pièce de monnaie), il suffit de multiplier les probabilités indépendantes entre elles. Par conséquent, la probabilité d'obtenir deux têtes lors de deux lancers d'une pièce est de ½ ½ = ¼. La probabilité d'obtenir un pile ou un face avec deux lancers est de ½, parce qu'il y a deux façons d'obtenir ce résultat : HT ou TH.

**Événements connexes.** Si les événements ne sont pas indépendants (par exemple, le premier événement est un nombre compris entre 1 et 3 inclus lors du lancer d'un dé à six faces et le second événement est un nombre pair), la règle de multiplication s'applique également, mais vous devez multiplier la probabilité d'un événement par la **probabilité conditionnelle** du second.

Lorsqu'on lance un dé, la probabilité indépendante d'un nombre compris entre 1 et 3 est de $3/6 = 1/2$, et la probabilité indépendante de tout nombre pair est également de $1/2$ (les nombres pairs sont 2, 4 ou 6 divisés par les six résultats possibles).

Cependant, si vous avez déjà lancé un nombre de 1 à 3, la probabilité que cet ensemble restreint de résultats soit un nombre pair est de $1/3$ (parce que "2" est le seul nombre pair possible dans cet ensemble de trois résultats). Par conséquent, la probabilité des **deux** événements liés est de $1/2 \times 1/3 = 1/6$. Vous pouvez examiner la situation dans l'autre sens : la probabilité d'obtenir un nombre pair en lançant un dé est de $1/2$ (vous obtiendriez les nombres 2, 4 ou 6) et la probabilité que l'un de ces nombres soit compris entre 1 et 3 est de $1/3$ (le nombre 2 parmi ces trois résultats). Par conséquent, la probabilité des deux résultats est à nouveau $1/2 \times 1/3 = 1/6$.
:::

| Nombre de boules noires | Nombre de boules blanches | Probabilité | Pourcentage de cas pouvant donner ce resultat |
|-----------------|-----------------|-----------------|----------------------|
| 6                       | 0                         | 1/64        | 1,56                                          |
| 5                       | 1                         | 6/64        | 9,38                                          |
| 4                       | 2                         | 15/64       | 23,44                                         |
| 3                       | 3                         | 20/64       | 31,25                                         |
| 2                       | 4                         | 15/64       | 23,44                                         |
| 1                       | 5                         | 6/64        | 9,38                                          |
| 0                       | 6                         | 1/64        | 1,56                                          |
| Total                   |                           | 64/64       | 100                                           |

![Les nombres attendus de chaque mélange possible de couleurs lors du tirage indépendant de six perles avec remplacement à 64 occasions différentes à partir d'une grande population contenant 50 % de noires et 50 % de blanches.](img/expected_number.png)

La probabilité de n'obtenir aucune boule blanche sur 6 boules tirées est de 1,56%. On rejette l'hypothèse nulle $H_0$ puisque $p < 2,5\%$ (test bidirectionnel).

### Passage intermédiaire par une statistique

Dans la majorité des cas, on ne peut pas calculer directement la p-valeur. L'alternative est de calculer une statistique qui quantifie la différence entre les données observées et les données attendues sous $H_0$. On calcule ensuite la p-valeur en comparant la valeur de la statistique à sa distribution théorique sous $H_0$.

#### Inférence à une moyenne

On peut par exemple être amené à comparer la moyenne d'un échantillon à la moyenne d'une population dont la variance est connue. Pour un tel test on a:

-   $H_0$: $\mu = \mu_0$; $H_A$: $\mu ≠ \mu_0$.\
-   Statistique de test: $Z_{obs} = \frac{\bar{Y} - \mu_0}{ESM} = \frac{\bar{Y} - \mu_0}{\frac{\sigma}{\sqrt{n}}} \sim N(0, 1)$

::: callout-note
Erreur standard à la moyenne (ESM): le terme "erreur" remplace celui de "déviation" quand on parle de la variation de moyennes plutôt que de la variation de mesures individuelles.
:::

#### Et si les paramètres sont inconnus ?

Si on ne connaît pas la variance réelle de la population, il faut tenir compte des biais possibles dus à la taille de l'échantillon (n).

On a alors:

-   Statistique de test ($S^2$ est la variance de l'échantillon): $t_{obs} = \frac{\bar{X} - \mu_0}{ESM} = \frac{\bar{X} - \mu_0}{\frac{S}{\sqrt{n -1 }}} \sim St(n-1)$.\
-   Degrés de liberté: nombre d'observations dans l'échantillon libres de varier quand on estime la variance.

Plus n est petit, plus la distribution est large car S sous-estime σ. Quand n = ∞, Student = Normale.

### Effet de la taille de l'échantillon (n)

Plus l'échantillon est grand, plus la précision avec laquelle est connue la moyenne est importante. Intervalle de confiance à la moyenne réelle $\mu$: $$ \bar{Y} \pm t_{1-\alpha/2; n-1} . \frac{S}{\sqrt{n-1}}$$.

## Test uni- *vs* bidirectionnel

L'hypothèse nulle est toujours "il n'existe pas de" différence/relation... Cela revient à dire qu'une certaine quantité est égale à 0. L'hypothèse alternative est soit:

-   Cette quantité est différente de 0 (test bidirectionnel);\
-   Cette quantité est supérieure/inférieure à 0 (test unidirectionnel).

Ainsi on a:

-   Test bidirectionnel:
    -   $H_0$ : $\mu = \mu_0$ ou $t = 0$
    -   $H_A$ : $\mu ≠ \mu_0$ ou $t ≠ 0$
-   Test unidirectionnel:
    -   $H_0$: $\mu = \mu_0$ ou $t = 0$
    -   $H_{A1}$: $\mu > \mu_0$ ou $t > 0$
    -   $H_{A2}$: $\mu < \mu_0$ ou $t < 0$

## Types d'erreurs

Quand on prend une décision concernqnt une hypothèse sur la base d'un échantillon donné, **on n'est jamais sûr de prendre la bonne décision!!**.

On peut se tromper de 2 manières:

-   $\alpha$ (probabilité d'erreur de type I: rejeter $H_0$ à tort) est traditionnellement fixé à 5%. Rien n'empêche de choisir un $\alpha$ plus petit ou plus grand;\
-   $1 - \beta$ (probabilité d'erreur de type II: accepter $H_0$ à tort) est la puissance du test. Elle dépend notamment de:
    -   La taille de l'effet: plus l'effet est grand, plus la probabilité de le détecter est grande;
    -   La variance: plus le paramètre est connu avec précision, plus la probabilité d'en détecter une différence est grande. La précision augmente avec le nombre d'observation.

## Compromis entre $\alpha$ et $\beta$

Il existe un compromis entre les probabilités d'erreur de type I ($\alpha$) et de type II ($\alpha$). Pour un même jeu de données, si on choisit de diminuer $\alpha$, on augmentera automatiquement $\beta$, et inversement.

## Les deux grands types de questions

On peut différencier en général 3 types de jeux de données en fonction du nombre de variables mesurées sur chaque individu:

-   Univarié: une seule variable;\
-   Bivarié: 2 variables;\
-   Multivarié: plus de 2 variables;

On peut différencier 2 grandes classes de questions:

-   L'hypothèse porte sur l'existence de différence(s);\
-   L'hypothèse porte sur l'existence d'une relation entre variables.

## Les grandes classes de méthodes d'analyse
